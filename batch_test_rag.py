#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æµ‹è¯•RAGç®¡é“è„šæœ¬ - Task 6 å‡çº§ç‰ˆ
æ”¯æŒåŸºçº¿å’Œå¢å¼ºæ¨¡å¼çš„A/Bå¯¹æ¯”æµ‹è¯•
"""

import json
import os
import time
import argparse
from datetime import datetime
from dotenv import load_dotenv
from tqdm import tqdm
from rag_pipeline import RAGPipeline
from enhanced_rag_pipeline import EnhancedRAGPipeline
import numpy as np

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def load_test_data(file_path):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def json_serializer(obj):
    """JSONåºåˆ—åŒ–è½¬æ¢å‡½æ•°ï¼Œå¤„ç†numpyæ•°æ®ç±»å‹"""
    if isinstance(obj, (np.integer, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')

def save_results(results, output_file):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=json_serializer)

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='RAGç®¡é“æ‰¹é‡æµ‹è¯•å·¥å…·')
    parser.add_argument('--mode', choices=['baseline', 'enhanced'], default='baseline',
                       help='é€‰æ‹©æµ‹è¯•æ¨¡å¼: baseline (åŸºçº¿RAG) æˆ– enhanced (å¢å¼ºRAG)')
    parser.add_argument('--test-file', default='test.json',
                       help='æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: test.json)')
    parser.add_argument('--output-dir', default='output',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: output)')
    args = parser.parse_args()
    
    print("=" * 60)
    print(f"Task 6: RAGç®¡é“æ‰¹é‡æµ‹è¯• - {args.mode.upper()} æ¨¡å¼")
    print("=" * 60)
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    test_file = args.test_file
    if not os.path.exists(test_file):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶ {test_file}")
        return
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print(f"æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    test_data = load_test_data(test_file)
    print(f"âœ“ åŠ è½½å®Œæˆï¼Œå…± {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    # åˆå§‹åŒ–æ€§èƒ½æ—¥å¿—
    log_entries = []
    start_time = datetime.now()
    log_entries.append(f"æ‰¹é‡æµ‹è¯•å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    log_entries.append(f"æµ‹è¯•æ¨¡å¼: {args.mode.upper()}")
    log_entries.append(f"æµ‹è¯•æ•°æ®é›†: {test_file}")
    log_entries.append(f"æ€»é—®é¢˜æ•°: {len(test_data)}")
    
    # åˆå§‹åŒ–RAGç®¡é“
    print(f"\næ­£åœ¨åˆå§‹åŒ–{args.mode.upper()}æ¨¡å¼RAGç®¡é“...")
    try:
        if args.mode == 'baseline':
            rag = RAGPipeline(
                index_path="output/knowledge_base.index",
                metadata_path="output/chunk_metadata.pkl",
                k=5
            )
            log_entries.append("ç®¡é“ç±»å‹: RAGPipeline (åŸºçº¿ç‰ˆ)")
            log_entries.append("é…ç½®: k=5")
        else:  # enhanced mode
            rag = EnhancedRAGPipeline(
                index_path="output/knowledge_base.index",
                metadata_path="output/chunk_metadata.pkl",
                initial_k=20,
                final_k=5,
                enable_reranking=True
            )
            log_entries.append("ç®¡é“ç±»å‹: EnhancedRAGPipeline (å¢å¼ºç‰ˆ)")
            log_entries.append("é…ç½®: initial_k=20, final_k=5, reranking=True")
        
        print(f"âœ“ {args.mode.upper()}æ¨¡å¼RAGç®¡é“åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âœ— RAGç®¡é“åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # æ‰¹é‡æµ‹è¯•
    print(f"\nå¼€å§‹æ‰¹é‡æµ‹è¯• {len(test_data)} ä¸ªé—®é¢˜...")
    results = []  # è¯¦ç»†æµ‹è¯•ç»“æœ
    submission_results = []  # submission.jsonæ ¼å¼ç»“æœ
    
    successful_count = 0
    failed_count = 0
    total_processing_time = 0
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
    for i, item in enumerate(tqdm(test_data, desc="å¤„ç†é—®é¢˜"), 1):
        question_start_time = time.time()
        
        try:
            # ä½¿ç”¨RAGç®¡é“å›ç­”é—®é¢˜
            result = rag.answer_question(item['question'])
            question_end_time = time.time()
            processing_time = question_end_time - question_start_time
            total_processing_time += processing_time
            
            # è®°å½•è¯¦ç»†ç»“æœ
            test_result = {
                "id": i,
                "filename": item['filename'],
                "page": item['page'],
                "question": item['question'],
                "expected_answer": item['answer'],
                "rag_answer": result['answer'],
                "sources": result['sources'],
                "processing_time": processing_time,
                "success": True
            }
            
            # è®°å½•submissionæ ¼å¼ç»“æœ
            submission_result = {
                "answer": result['answer'],
                "filename": item['filename'],
                "page": item['page']
            }
            
            successful_count += 1
            log_entries.append(f"é—®é¢˜ {i}: æˆåŠŸ (è€—æ—¶: {processing_time:.2f}s)")
            
        except Exception as e:
            question_end_time = time.time()
            processing_time = question_end_time - question_start_time
            total_processing_time += processing_time
            
            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
            
            # è®°å½•è¯¦ç»†ç»“æœ
            test_result = {
                "id": i,
                "filename": item['filename'],
                "page": item['page'],
                "question": item['question'],
                "expected_answer": item['answer'],
                "rag_answer": error_msg,
                "sources": [],
                "processing_time": processing_time,
                "success": False
            }
            
            # è®°å½•submissionæ ¼å¼ç»“æœï¼ˆå¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            submission_result = {
                "answer": "æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚",
                "filename": item['filename'],
                "page": item['page']
            }
            
            failed_count += 1
            log_entries.append(f"é—®é¢˜ {i}: å¤±è´¥ - {str(e)} (è€—æ—¶: {processing_time:.2f}s)")
        
        results.append(test_result)
        submission_results.append(submission_result)
    
    # è®¡ç®—æ€»ä½“æ€§èƒ½æŒ‡æ ‡
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()
    avg_processing_time = total_processing_time / len(test_data) if test_data else 0
    success_rate = (successful_count / len(test_data) * 100) if test_data else 0
    
    # æ·»åŠ æ€§èƒ½ç»Ÿè®¡åˆ°æ—¥å¿—
    log_entries.extend([
        f"æ‰¹é‡æµ‹è¯•ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}",
        f"æ€»è€—æ—¶: {total_duration:.2f}ç§’",
        f"æˆåŠŸå›ç­”æ•°: {successful_count}",
        f"å¤±è´¥æ•°: {failed_count}",
        f"æˆåŠŸç‡: {success_rate:.1f}%",
        f"å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.2f}ç§’/é—®é¢˜",
        f"æ€»å¤„ç†æ—¶é—´: {total_processing_time:.2f}ç§’"
    ])
    
    # ä¿å­˜submission.jsonæ–‡ä»¶
    submission_file = f"{args.output_dir}/submission_{args.mode}.json"
    print(f"\næ­£åœ¨ä¿å­˜æäº¤æ–‡ä»¶: {submission_file}")
    with open(submission_file, 'w', encoding='utf-8') as f:
        json.dump(submission_results, f, ensure_ascii=False, indent=2, default=json_serializer)
    print(f"âœ“ æäº¤æ–‡ä»¶ä¿å­˜å®Œæˆ")
    
    # ä¿å­˜è¯¦ç»†æµ‹è¯•ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    detailed_results_file = f"{args.output_dir}/rag_test_results_{args.mode}_{timestamp}.json"
    print(f"\næ­£åœ¨ä¿å­˜è¯¦ç»†æµ‹è¯•ç»“æœ: {detailed_results_file}")
    save_results(results, detailed_results_file)
    print(f"âœ“ è¯¦ç»†ç»“æœä¿å­˜å®Œæˆ")
    
    # ä¿å­˜æ€§èƒ½æ—¥å¿—
    log_file = f"{args.output_dir}/{args.mode}_performance_log.txt"
    print(f"\næ­£åœ¨ä¿å­˜æ€§èƒ½æ—¥å¿—: {log_file}")
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write("\n".join(log_entries))
    print(f"âœ“ æ€§èƒ½æ—¥å¿—ä¿å­˜å®Œæˆ")
    
    # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 60)
    print(f"Task 6 æ‰¹é‡æµ‹è¯•å®Œæˆç»Ÿè®¡ - {args.mode.upper()} æ¨¡å¼")
    print("=" * 60)
    print(f"æµ‹è¯•æ¨¡å¼: {args.mode.upper()}")
    print(f"æ€»æµ‹è¯•æ•°é‡: {len(results)}")
    print(f"æˆåŠŸæ•°é‡: {successful_count}")
    print(f"å¤±è´¥æ•°é‡: {failed_count}")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"æ€»è€—æ—¶: {total_duration:.2f}ç§’")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.2f}ç§’/é—®é¢˜")
    
    # æ˜¾ç¤ºå¢å¼ºæ¨¡å¼ç‰¹æœ‰çš„æ€§èƒ½æŒ‡æ ‡
    if args.mode == 'enhanced' and hasattr(rag, 'get_performance_report'):
        print("\nå¢å¼ºæ¨¡å¼æ€§èƒ½æŠ¥å‘Š:")
        report = rag.get_performance_report()
        for key, value in report.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for sub_key, sub_value in value.items():
                    print(f"    {sub_key}: {sub_value}")
            elif isinstance(value, float):
                print(f"  {key}: {value:.3f}")
            else:
                print(f"  {key}: {value}")
    
    print(f"\nç”Ÿæˆæ–‡ä»¶:")
    print(f"  - æäº¤æ–‡ä»¶: {submission_file}")
    print(f"  - è¯¦ç»†ç»“æœ: {detailed_results_file}")
    print(f"  - æ€§èƒ½æ—¥å¿—: {log_file}")
    print("=" * 60)
    
    # æç¤ºç”¨æˆ·å¦‚ä½•è¿›è¡Œå¯¹æ¯”æµ‹è¯•
    if args.mode == 'baseline':
        print("\nğŸ’¡ æç¤º: è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œå¢å¼ºæ¨¡å¼æµ‹è¯•ä»¥ä¾¿å¯¹æ¯”:")
        print(f"   python {os.path.basename(__file__)} --mode enhanced")
    elif args.mode == 'enhanced':
        print("\nğŸ’¡ æç¤º: å¦‚éœ€å¯¹æ¯”ï¼Œè¯·ç¡®ä¿å·²è¿è¡ŒåŸºçº¿æ¨¡å¼æµ‹è¯•:")
        print(f"   python {os.path.basename(__file__)} --mode baseline")
    print("=" * 60)

if __name__ == "__main__":
    main()