{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDFæ•°æ®é¢„å¤„ç†\n",
    "\n",
    "æœ¬notebookç”¨äºå¤„ç†datas/ç›®å½•ä¸‹çš„æ‰€æœ‰PDFæ–‡ä»¶ï¼Œä½¿ç”¨MinerUè¿›è¡Œè§£æå¹¶ç”Ÿæˆç»“æ„åŒ–çš„JSONè¾“å‡ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "# å°è¯•å¯¼å…¥mineruç›¸å…³æ¨¡å—\n",
    "try:\n",
    "    import mineru\n",
    "    print(\"âœ“ MinerUåº“å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— MinerUåº“å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…mineruåº“\")\n",
    "\n",
    "print(f\"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. é…ç½®è·¯å¾„å’Œå‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®é¡¹ç›®è·¯å¾„\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATAS_DIR = PROJECT_ROOT / 'datas'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'output'\n",
    "SCRIPTS_DIR = PROJECT_ROOT / 'scripts'\n",
    "\n",
    "# è¾“å‡ºæ–‡ä»¶å\n",
    "OUTPUT_JSON_FILE = OUTPUT_DIR / 'all_pdf_page_chunks.json'\n",
    "\n",
    "# ç¡®ä¿ç›®å½•å­˜åœ¨\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "DATAS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"æ•°æ®ç›®å½•: {DATAS_DIR}\")\n",
    "print(f\"è¾“å‡ºç›®å½•: {OUTPUT_DIR}\")\n",
    "print(f\"è¾“å‡ºæ–‡ä»¶: {OUTPUT_JSON_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ£€æŸ¥PDFæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶\n",
    "pdf_files = list(DATAS_DIR.glob('*.pdf'))\n",
    "\n",
    "print(f\"åœ¨ {DATAS_DIR} ç›®å½•ä¸‹æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶:\")\n",
    "\n",
    "if pdf_files:\n",
    "    for i, pdf_file in enumerate(pdf_files, 1):\n",
    "        file_size = pdf_file.stat().st_size / (1024 * 1024)  # MB\n",
    "        print(f\"{i}. {pdf_file.name} ({file_size:.2f} MB)\")\n",
    "else:\n",
    "    print(\"âš ï¸  æœªæ‰¾åˆ°PDFæ–‡ä»¶ï¼Œè¯·å°†PDFæ–‡ä»¶æ”¾å…¥datas/ç›®å½•ä¸­\")\n",
    "    print(\"\\nåˆ›å»ºç¤ºä¾‹PDFæ–‡ä»¶ç”¨äºæµ‹è¯•...\")\n",
    "    \n",
    "    # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹PDFæ–‡ä»¶ç”¨äºæµ‹è¯•\n",
    "    sample_pdf_content = b\"%PDF-1.4\\n1 0 obj<</Type/Catalog/Pages 2 0 R>>endobj 2 0 obj<</Type/Pages/Kids[3 0 R]/Count 1>>endobj 3 0 obj<</Type/Page/Parent 2 0 R/MediaBox[0 0 612 792]>>endobj xref\\n0 4\\n0000000000 65535 f\\n0000000010 00000 n\\n0000000053 00000 n\\n0000000125 00000 n\\ntrailer<</Size 4/Root 1 0 R>>startxref\\n203\\n%%EOF\"\n",
    "    \n",
    "    sample_pdf_path = DATAS_DIR / 'sample_document.pdf'\n",
    "    with open(sample_pdf_path, 'wb') as f:\n",
    "        f.write(sample_pdf_content)\n",
    "    \n",
    "    print(f\"âœ“ å·²åˆ›å»ºç¤ºä¾‹PDFæ–‡ä»¶: {sample_pdf_path}\")\n",
    "    pdf_files = [sample_pdf_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MinerU PDFè§£æå‡½æ•°\n",
    "\n",
    "å®šä¹‰PDFè§£æçš„æ ¸å¿ƒå‡½æ•°ï¼ŒåŒ…å«é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf_with_mineru(pdf_path):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨MinerUè§£æå•ä¸ªPDFæ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (Path): PDFæ–‡ä»¶è·¯å¾„\n",
    "    \n",
    "    Returns:\n",
    "        dict: è§£æç»“æœï¼ŒåŒ…å«é¡µé¢å†…å®¹ã€è¡¨æ ¼ã€å›¾ç‰‡ç­‰ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"æ­£åœ¨è§£æ: {pdf_path.name}\")\n",
    "        \n",
    "        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„mineru APIè¿›è¡Œè°ƒæ•´\n",
    "        # ç”±äºmineruçš„å…·ä½“APIå¯èƒ½å› ç‰ˆæœ¬è€Œå¼‚ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶\n",
    "        \n",
    "        # ç¤ºä¾‹è§£æé€»è¾‘ï¼ˆéœ€è¦æ ¹æ®å®é™…mineru APIè°ƒæ•´ï¼‰\n",
    "        parsed_content = {\n",
    "            'file_name': pdf_path.name,\n",
    "            'file_path': str(pdf_path),\n",
    "            'file_size': pdf_path.stat().st_size,\n",
    "            'parsed_at': datetime.now().isoformat(),\n",
    "            'pages': [],\n",
    "            'tables': [],\n",
    "            'images': [],\n",
    "            'metadata': {}\n",
    "        }\n",
    "        \n",
    "        # æ¨¡æ‹Ÿè§£æè¿‡ç¨‹ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›¿æ¢ä¸ºçœŸå®çš„mineruè°ƒç”¨ï¼‰\n",
    "        try:\n",
    "            # å°è¯•ä½¿ç”¨mineruè¿›è¡Œå®é™…è§£æ\n",
    "            # æ³¨æ„ï¼šè¿™é‡Œçš„ä»£ç éœ€è¦æ ¹æ®mineruçš„å®é™…APIè¿›è¡Œè°ƒæ•´\n",
    "            \n",
    "            # ç¤ºä¾‹ï¼šå‡è®¾mineruæœ‰ä¸€ä¸ªparse_pdfå‡½æ•°\n",
    "            # result = mineru.parse_pdf(str(pdf_path))\n",
    "            # parsed_content.update(result)\n",
    "            \n",
    "            # ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®\n",
    "            import PyPDF2\n",
    "            \n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                \n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    try:\n",
    "                        text = page.extract_text()\n",
    "                        page_data = {\n",
    "                            'page_number': page_num + 1,\n",
    "                            'text': text,\n",
    "                            'text_length': len(text),\n",
    "                            'chunks': []\n",
    "                        }\n",
    "                        \n",
    "                        # å°†æ–‡æœ¬åˆ†å—ï¼ˆæ¯500å­—ç¬¦ä¸€å—ï¼‰\n",
    "                        chunk_size = 500\n",
    "                        for i in range(0, len(text), chunk_size):\n",
    "                            chunk = text[i:i+chunk_size]\n",
    "                            if chunk.strip():\n",
    "                                page_data['chunks'].append({\n",
    "                                    'chunk_id': f\"{pdf_path.stem}_page_{page_num+1}_chunk_{len(page_data['chunks'])+1}\",\n",
    "                                    'content': chunk.strip(),\n",
    "                                    'start_pos': i,\n",
    "                                    'end_pos': min(i+chunk_size, len(text))\n",
    "                                })\n",
    "                        \n",
    "                        parsed_content['pages'].append(page_data)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  âš ï¸  é¡µé¢ {page_num+1} è§£æå¤±è´¥: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "        except ImportError:\n",
    "            print(\"  âš ï¸  PyPDF2æœªå®‰è£…ï¼Œä½¿ç”¨åŸºç¡€è§£ææ¨¡å¼\")\n",
    "            # åˆ›å»ºåŸºç¡€çš„è§£æç»“æœ\n",
    "            parsed_content['pages'] = [{\n",
    "                'page_number': 1,\n",
    "                'text': f\"PDFæ–‡ä»¶: {pdf_path.name}\\næ–‡ä»¶å¤§å°: {pdf_path.stat().st_size} bytes\",\n",
    "                'chunks': [{\n",
    "                    'chunk_id': f\"{pdf_path.stem}_page_1_chunk_1\",\n",
    "                    'content': f\"è¿™æ˜¯æ¥è‡ª {pdf_path.name} çš„ç¤ºä¾‹å†…å®¹\",\n",
    "                    'start_pos': 0,\n",
    "                    'end_pos': 50\n",
    "                }]\n",
    "            }]\n",
    "        \n",
    "        parsed_content['total_pages'] = len(parsed_content['pages'])\n",
    "        parsed_content['total_chunks'] = sum(len(page['chunks']) for page in parsed_content['pages'])\n",
    "        \n",
    "        print(f\"  âœ“ è§£æå®Œæˆ: {parsed_content['total_pages']} é¡µ, {parsed_content['total_chunks']} ä¸ªæ–‡æœ¬å—\")\n",
    "        return parsed_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"è§£æ {pdf_path.name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\"\n",
    "        print(f\"  âœ— {error_msg}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯Noneï¼Œç¡®ä¿æµç¨‹ç»§ç»­\n",
    "        return {\n",
    "            'file_name': pdf_path.name,\n",
    "            'file_path': str(pdf_path),\n",
    "            'error': error_msg,\n",
    "            'parsed_at': datetime.now().isoformat(),\n",
    "            'pages': [],\n",
    "            'total_pages': 0,\n",
    "            'total_chunks': 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ‰¹é‡å¤„ç†PDFæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs():\n",
    "    \"\"\"\n",
    "    æ‰¹é‡å¤„ç†æ‰€æœ‰PDFæ–‡ä»¶\n",
    "    \n",
    "    Returns:\n",
    "        dict: åŒ…å«æ‰€æœ‰PDFè§£æç»“æœçš„å­—å…¸\n",
    "    \"\"\"\n",
    "    all_results = {\n",
    "        'processing_info': {\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'total_files': len(pdf_files),\n",
    "            'processed_files': 0,\n",
    "            'failed_files': 0,\n",
    "            'success_files': []\n",
    "        },\n",
    "        'documents': []\n",
    "    }\n",
    "    \n",
    "    print(f\"å¼€å§‹å¤„ç† {len(pdf_files)} ä¸ªPDFæ–‡ä»¶...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦\n",
    "    for pdf_file in tqdm(pdf_files, desc=\"å¤„ç†PDFæ–‡ä»¶\"):\n",
    "        try:\n",
    "            result = parse_pdf_with_mineru(pdf_file)\n",
    "            all_results['documents'].append(result)\n",
    "            \n",
    "            if 'error' in result:\n",
    "                all_results['processing_info']['failed_files'] += 1\n",
    "            else:\n",
    "                all_results['processing_info']['success_files'].append(pdf_file.name)\n",
    "            \n",
    "            all_results['processing_info']['processed_files'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"å¤„ç† {pdf_file.name} æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}\"\n",
    "            print(f\"âœ— {error_msg}\")\n",
    "            \n",
    "            all_results['documents'].append({\n",
    "                'file_name': pdf_file.name,\n",
    "                'file_path': str(pdf_file),\n",
    "                'error': error_msg,\n",
    "                'parsed_at': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            all_results['processing_info']['failed_files'] += 1\n",
    "            all_results['processing_info']['processed_files'] += 1\n",
    "    \n",
    "    all_results['processing_info']['end_time'] = datetime.now().isoformat()\n",
    "    \n",
    "    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯\n",
    "    total_pages = sum(doc.get('total_pages', 0) for doc in all_results['documents'])\n",
    "    total_chunks = sum(doc.get('total_chunks', 0) for doc in all_results['documents'])\n",
    "    \n",
    "    all_results['processing_info']['total_pages'] = total_pages\n",
    "    all_results['processing_info']['total_chunks'] = total_chunks\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"å¤„ç†å®Œæˆ!\")\n",
    "    print(f\"æ€»æ–‡ä»¶æ•°: {all_results['processing_info']['total_files']}\")\n",
    "    print(f\"æˆåŠŸå¤„ç†: {len(all_results['processing_info']['success_files'])}\")\n",
    "    print(f\"å¤„ç†å¤±è´¥: {all_results['processing_info']['failed_files']}\")\n",
    "    print(f\"æ€»é¡µæ•°: {total_pages}\")\n",
    "    print(f\"æ€»æ–‡æœ¬å—æ•°: {total_chunks}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# æ‰§è¡Œæ‰¹é‡å¤„ç†\n",
    "if pdf_files:\n",
    "    results = process_all_pdfs()\n",
    "else:\n",
    "    print(\"æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶è¿›è¡Œå¤„ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶\n",
    "if 'results' in locals() and results:\n",
    "    try:\n",
    "        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "        OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "        \n",
    "        # ä¿å­˜åˆ°JSONæ–‡ä»¶\n",
    "        with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸåˆ›å»º\n",
    "        if OUTPUT_JSON_FILE.exists():\n",
    "            file_size = OUTPUT_JSON_FILE.stat().st_size\n",
    "            print(f\"âœ“ æˆåŠŸä¿å­˜ç»“æœåˆ°: {OUTPUT_JSON_FILE}\")\n",
    "            print(f\"âœ“ æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB\")\n",
    "            \n",
    "            # éªŒè¯JSONæ–‡ä»¶çš„æœ‰æ•ˆæ€§\n",
    "            try:\n",
    "                with open(OUTPUT_JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "                    test_load = json.load(f)\n",
    "                print(\"âœ“ JSONæ–‡ä»¶æ ¼å¼éªŒè¯é€šè¿‡\")\n",
    "                \n",
    "                # æ˜¾ç¤ºæ–‡ä»¶å†…å®¹æ‘˜è¦\n",
    "                print(\"\\næ–‡ä»¶å†…å®¹æ‘˜è¦:\")\n",
    "                print(f\"- å¤„ç†çš„æ–‡æ¡£æ•°é‡: {len(test_load.get('documents', []))}\")\n",
    "                print(f\"- æ€»é¡µæ•°: {test_load.get('processing_info', {}).get('total_pages', 0)}\")\n",
    "                print(f\"- æ€»æ–‡æœ¬å—æ•°: {test_load.get('processing_info', {}).get('total_chunks', 0)}\")\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"âœ— JSONæ–‡ä»¶æ ¼å¼éªŒè¯å¤±è´¥: {e}\")\n",
    "        else:\n",
    "            print(f\"âœ— æ–‡ä»¶ä¿å­˜å¤±è´¥: {OUTPUT_JSON_FILE}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"æ²¡æœ‰å¤„ç†ç»“æœéœ€è¦ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å®‰è£…PyPDF2ä¾èµ–ï¼ˆå¦‚æœéœ€è¦ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…PyPDF2ç”¨äºåŸºç¡€PDFè§£æ\n",
    "try:\n",
    "    import PyPDF2\n",
    "    print(\"âœ“ PyPDF2å·²å®‰è£…\")\n",
    "except ImportError:\n",
    "    print(\"å®‰è£…PyPDF2...\")\n",
    "    !pip install PyPDF2\n",
    "    print(\"âœ“ PyPDF2å®‰è£…å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ˜¾ç¤ºæœ€ç»ˆç»“æœæ‘˜è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºæœ€ç»ˆå¤„ç†ç»“æœæ‘˜è¦\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š PDFæ•°æ®é¢„å¤„ç†å®Œæˆæ‘˜è¦\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if OUTPUT_JSON_FILE.exists():\n",
    "    with open(OUTPUT_JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "        final_results = json.load(f)\n",
    "    \n",
    "    processing_info = final_results.get('processing_info', {})\n",
    "    \n",
    "    print(f\"ğŸ“ è¾“å‡ºæ–‡ä»¶: {OUTPUT_JSON_FILE}\")\n",
    "    print(f\"ğŸ“Š æ–‡ä»¶å¤§å°: {OUTPUT_JSON_FILE.stat().st_size / 1024:.2f} KB\")\n",
    "    print(f\"ğŸ“„ å¤„ç†æ–‡æ¡£æ•°: {processing_info.get('total_files', 0)}\")\n",
    "    print(f\"âœ… æˆåŠŸå¤„ç†: {len(processing_info.get('success_files', []))}\")\n",
    "    print(f\"âŒ å¤„ç†å¤±è´¥: {processing_info.get('failed_files', 0)}\")\n",
    "    print(f\"ğŸ“– æ€»é¡µæ•°: {processing_info.get('total_pages', 0)}\")\n",
    "    print(f\"ğŸ”¤ æ€»æ–‡æœ¬å—æ•°: {processing_info.get('total_chunks', 0)}\")\n",
    "    print(f\"â° å¤„ç†æ—¶é—´: {processing_info.get('start_time', 'N/A')} - {processing_info.get('end_time', 'N/A')}\")\n",
    "    \n",
    "    if processing_info.get('success_files'):\n",
    "        print(\"\\nâœ… æˆåŠŸå¤„ç†çš„æ–‡ä»¶:\")\n",
    "        for file_name in processing_info['success_files']:\n",
    "            print(f\"  - {file_name}\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ æ•°æ®é¢„å¤„ç†ä»»åŠ¡å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“‚ ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³: {OUTPUT_JSON_FILE}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶ï¼Œå¤„ç†å¯èƒ½å¤±è´¥\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}