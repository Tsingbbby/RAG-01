#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG Pipeline - æ— faissä¾èµ–ç‰ˆæœ¬
ä½¿ç”¨numpyå®ç°å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ï¼Œæ›¿ä»£faissç´¢å¼•
"""

import os
import json
import numpy as np
from typing import Dict, Any, List
from dotenv import load_dotenv

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from embedding_module import EmbeddingAPIClient
from llm_api_client import GenerationAPIClient

class SimpleVectorIndex:
    """ç®€å•å‘é‡ç´¢å¼•ï¼Œä½¿ç”¨numpyå®ç°ç›¸ä¼¼åº¦æœç´¢"""
    
    def __init__(self):
        self.vectors = None
        self.dimension = None
        self.ntotal = 0
    
    def add(self, vectors: np.ndarray):
        """æ·»åŠ å‘é‡åˆ°ç´¢å¼•"""
        if self.vectors is None:
            self.vectors = vectors.copy()
            self.dimension = vectors.shape[1]
        else:
            self.vectors = np.vstack([self.vectors, vectors])
        self.ntotal = self.vectors.shape[0]
    
    def search(self, query_vectors: np.ndarray, k: int = 5):
        """æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡"""
        if self.vectors is None or self.ntotal == 0:
            return np.array([]), np.array([])
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        query_norm = np.linalg.norm(query_vectors, axis=1, keepdims=True)
        vectors_norm = np.linalg.norm(self.vectors, axis=1, keepdims=True)
        
        # é¿å…é™¤é›¶é”™è¯¯
        query_norm = np.where(query_norm == 0, 1, query_norm)
        vectors_norm = np.where(vectors_norm == 0, 1, vectors_norm)
        
        # å½’ä¸€åŒ–å‘é‡
        query_normalized = query_vectors / query_norm
        vectors_normalized = self.vectors / vectors_norm
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarities = np.dot(query_normalized, vectors_normalized.T)
        
        # è½¬æ¢ä¸ºè·ç¦»ï¼ˆ1 - ç›¸ä¼¼åº¦ï¼‰
        distances = 1 - similarities
        
        # è·å–top-kç»“æœ
        k = min(k, self.ntotal)
        indices = np.argsort(distances, axis=1)[:, :k]
        
        # è·å–å¯¹åº”çš„è·ç¦»
        batch_size = distances.shape[0]
        selected_distances = np.array([
            distances[i, indices[i]] for i in range(batch_size)
        ])
        
        return selected_distances, indices

class RAGPipelineNoFaiss:
    """RAGæ£€ç´¢å¢å¼ºç”Ÿæˆç®¡é“ - æ— faissä¾èµ–ç‰ˆæœ¬"""
    
    def __init__(self, index_file: str, metadata_file: str):
        """
        åˆå§‹åŒ–RAGç®¡é“
        
        Args:
            index_file: å‘é‡ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆ.npyæ ¼å¼ï¼‰
            metadata_file: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆ.jsonæ ¼å¼ï¼‰
        """
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        self.embedding_client = EmbeddingAPIClient()
        self.generation_client = GenerationAPIClient()
        
        # åŠ è½½å‘é‡ç´¢å¼•å’Œå…ƒæ•°æ®
        self.index = SimpleVectorIndex()
        self.metadata = []
        
        self._load_index_and_metadata(index_file, metadata_file)
        
        print(f"âœ… RAGç®¡é“åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š å‘é‡ç´¢å¼•å¤§å°: {self.index.ntotal}")
        print(f"ğŸ“‹ å…ƒæ•°æ®æ¡ç›®: {len(self.metadata)}")
    
    def _load_index_and_metadata(self, index_file: str, metadata_file: str):
        """åŠ è½½å‘é‡ç´¢å¼•å’Œå…ƒæ•°æ®"""
        try:
            # åŠ è½½å‘é‡æ•°æ®
            if os.path.exists(index_file):
                vectors = np.load(index_file)
                self.index.add(vectors)
                print(f"âœ… å‘é‡ç´¢å¼•åŠ è½½æˆåŠŸ: {vectors.shape}")
            else:
                raise FileNotFoundError(f"å‘é‡ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file}")
            
            # åŠ è½½å…ƒæ•°æ®
            if os.path.exists(metadata_file):
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                print(f"âœ… å…ƒæ•°æ®åŠ è½½æˆåŠŸ: {len(self.metadata)} æ¡è®°å½•")
            else:
                raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
            
            # éªŒè¯æ•°æ®ä¸€è‡´æ€§
            if self.index.ntotal != len(self.metadata):
                raise ValueError(
                    f"å‘é‡æ•°é‡({self.index.ntotal})ä¸å…ƒæ•°æ®æ•°é‡({len(self.metadata)})ä¸åŒ¹é…"
                )
                
        except Exception as e:
            print(f"âŒ åŠ è½½ç´¢å¼•å’Œå…ƒæ•°æ®å¤±è´¥: {e}")
            raise
    
    def answer_question(self, question: str, top_k: int = 5, max_context_length: int = 3000) -> Dict[str, Any]:
        """
        å›ç­”é—®é¢˜çš„ä¸»è¦æ–¹æ³•
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            
        Returns:
            åŒ…å«ç­”æ¡ˆã€æ¥æºå’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        """
        try:
            print(f"\nğŸ” å¼€å§‹å¤„ç†é—®é¢˜: {question}")
            
            # 1. å°†é—®é¢˜è½¬æ¢ä¸ºå‘é‡
            print("ğŸ“ æ­£åœ¨ç”Ÿæˆé—®é¢˜å‘é‡...")
            question_vector = self.embedding_client.get_embeddings([question])
            question_vector = np.array(question_vector).reshape(1, -1)
            
            # 2. åœ¨å‘é‡ç´¢å¼•ä¸­æœç´¢ç›¸ä¼¼æ–‡æ¡£
            print(f"ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£ (top_k={top_k})...")
            distances, indices = self.index.search(question_vector, k=top_k)
            
            # 3. è·å–æ£€ç´¢åˆ°çš„æ–‡æ¡£
            retrieved_docs = []
            for i, idx in enumerate(indices[0]):
                doc_info = self.metadata[idx].copy()
                doc_info['similarity_score'] = float(1 - distances[0][i])  # è½¬æ¢å›ç›¸ä¼¼åº¦
                doc_info['rank'] = i + 1
                retrieved_docs.append(doc_info)
            
            print(f"ğŸ“š æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # 4. æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(retrieved_docs, max_context_length)
            
            # 5. ç”Ÿæˆç­”æ¡ˆ
            print("ğŸ¤– æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
            answer = self._generate_answer(question, context, retrieved_docs)
            
            # 6. æ„å»ºè¿”å›ç»“æœ
            result = {
                'question': question,
                'answer': answer,
                'sources': retrieved_docs,
                'context_used': context,
                'retrieval_stats': {
                    'total_docs_in_index': self.index.ntotal,
                    'docs_retrieved': len(retrieved_docs),
                    'top_similarity': float(retrieved_docs[0]['similarity_score']) if retrieved_docs else 0.0
                }
            }
            
            print("âœ… é—®é¢˜å¤„ç†å®Œæˆ")
            return result
            
        except Exception as e:
            print(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
            return {
                'question': question,
                'answer': f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                'sources': [],
                'error': str(e)
            }
    
    def _build_context(self, retrieved_docs: List[Dict], max_length: int) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        context_parts = []
        current_length = 0
        
        for doc in retrieved_docs:
            content = doc.get('content', '')
            source_info = f"[æ¥æº: {doc.get('source', 'æœªçŸ¥')}]"
            
            part = f"{source_info}\n{content}\n"
            
            if current_length + len(part) > max_length:
                break
                
            context_parts.append(part)
            current_length += len(part)
        
        return "\n".join(context_parts)
    
    def _generate_answer(self, question: str, context: str, sources: List[Dict]) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»ºæç¤ºè¯
        source_list = "\n".join([
            f"{i+1}. {doc.get('source', 'æœªçŸ¥æ¥æº')} (ç›¸ä¼¼åº¦: {doc.get('similarity_score', 0):.3f})"
            for i, doc in enumerate(sources[:3])
        ])
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å¼•ç”¨å…·ä½“çš„æ¥æºä¿¡æ¯
4. ä¿æŒç­”æ¡ˆå‡†ç¡®ã€ç®€æ´

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ä¸»è¦æ¥æºï¼š
{source_list}

é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†çš„ç­”æ¡ˆï¼š"""
        
        try:
            response = self.generation_client.generate(prompt)
            return response
        except Exception as e:
            return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}"
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç®¡é“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'index_size': self.index.ntotal,
            'metadata_count': len(self.metadata),
            'vector_dimension': self.index.dimension,
            'embedding_client_ready': hasattr(self.embedding_client, 'base_url'),
            'generation_client_ready': hasattr(self.generation_client, 'base_url')
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    index_file = "output/document_vectors.npy"
    metadata_file = "output/document_metadata.json"
    
    try:
        # åˆå§‹åŒ–RAGç®¡é“
        rag = RAGPipelineNoFaiss(index_file, metadata_file)
        
        # æµ‹è¯•é—®é¢˜
        test_question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
        result = rag.answer_question(test_question)
        
        print("\n" + "="*50)
        print(f"é—®é¢˜: {result['question']}")
        print(f"ç­”æ¡ˆ: {result['answer']}")
        print(f"æ¥æºæ•°é‡: {len(result['sources'])}")
        
    except Exception as e:
        print(f"è¿è¡Œç¤ºä¾‹æ—¶å‡ºé”™: {e}")