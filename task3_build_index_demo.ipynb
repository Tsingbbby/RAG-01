{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: 构建与索引向量知识库\n",
    "\n",
    "本Notebook演示如何使用Task 3的功能将Task 2生成的向量化知识块构建成可进行高效相似度搜索的向量知识库。\n",
    "\n",
    "## 主要功能\n",
    "\n",
    "1. **数据加载**: 读取`vectorized_chunks.jsonl`文件\n",
    "2. **索引构建**: 使用FAISS构建高效的向量索引\n",
    "3. **GPU加速**: 自动检测并使用GPU加速索引构建\n",
    "4. **文件保存**: 生成`knowledge_base.index`和`chunk_metadata.pkl`文件\n",
    "\n",
    "## 输出文件\n",
    "\n",
    "- `output/knowledge_base.index`: FAISS二进制索引文件\n",
    "- `output/chunk_metadata.pkl`: 元数据映射文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境检查与依赖导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 检查必要的库\n",
    "try:\n",
    "    import faiss\n",
    "    print(f\"✅ FAISS版本: {faiss.__version__}\")\n",
    "    \n",
    "    # 检查GPU支持\n",
    "    gpu_count = faiss.get_num_gpus()\n",
    "    if gpu_count > 0:\n",
    "        print(f\"🚀 检测到 {gpu_count} 个GPU，将使用GPU加速\")\n",
    "    else:\n",
    "        print(\"💻 未检测到GPU，将使用CPU构建索引\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ 请先安装faiss-gpu: pip install faiss-gpu\")\n",
    "\n",
    "# 检查输入文件\n",
    "input_file = \"output/vectorized_chunks.jsonl\"\n",
    "if os.path.exists(input_file):\n",
    "    file_size = os.path.getsize(input_file) / (1024 * 1024)  # MB\n",
    "    print(f\"✅ 输入文件存在: {input_file} ({file_size:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"❌ 输入文件不存在: {input_file}\")\n",
    "    print(\"请先运行Task 2生成向量化数据文件\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入构建器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_index import VectorKnowledgeBaseBuilder\n",
    "\n",
    "# 创建构建器实例\n",
    "builder = VectorKnowledgeBaseBuilder(\n",
    "    input_file=\"output/vectorized_chunks.jsonl\",\n",
    "    output_dir=\"output\"\n",
    ")\n",
    "\n",
    "print(\"构建器已初始化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 执行向量知识库构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行完整的构建流程\n",
    "try:\n",
    "    print(\"开始构建向量知识库...\")\n",
    "    builder.build()\n",
    "    print(\"\\n🎉 向量知识库构建完成!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 构建失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 验证输出文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证输出文件\n",
    "index_file = \"output/knowledge_base.index\"\n",
    "metadata_file = \"output/chunk_metadata.pkl\"\n",
    "\n",
    "print(\"=== 输出文件验证 ===\")\n",
    "\n",
    "# 检查文件存在性\n",
    "if os.path.exists(index_file):\n",
    "    size_mb = os.path.getsize(index_file) / (1024 * 1024)\n",
    "    print(f\"✅ 索引文件: {index_file} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"❌ 索引文件不存在: {index_file}\")\n",
    "\n",
    "if os.path.exists(metadata_file):\n",
    "    size_mb = os.path.getsize(metadata_file) / (1024 * 1024)\n",
    "    print(f\"✅ 元数据文件: {metadata_file} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"❌ 元数据文件不存在: {metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 测试搜索功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试搜索功能\n",
    "try:\n",
    "    import faiss\n",
    "    import pickle\n",
    "    \n",
    "    # 加载索引和元数据\n",
    "    index = faiss.read_index(\"output/knowledge_base.index\")\n",
    "    with open(\"output/chunk_metadata.pkl\", \"rb\") as f:\n",
    "        metadata_list = pickle.load(f)\n",
    "    \n",
    "    print(f\"📊 索引信息:\")\n",
    "    print(f\"   向量数量: {index.ntotal:,}\")\n",
    "    print(f\"   向量维度: {index.d}\")\n",
    "    print(f\"   元数据条目数: {len(metadata_list):,}\")\n",
    "    \n",
    "    # 测试搜索\n",
    "    if index.ntotal > 0:\n",
    "        query_vector = index.reconstruct(0).reshape(1, -1)\n",
    "        distances, indices = index.search(query_vector, k=3)\n",
    "        \n",
    "        print(f\"\\n🔍 搜索测试结果:\")\n",
    "        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            metadata = metadata_list[idx]\n",
    "            print(f\"  [{i+1}] 距离: {dist:.6f}, chunk_id: {metadata['chunk_id']}\")\n",
    "        \n",
    "        if distances[0][0] < 1e-6:\n",
    "            print(\"✅ 搜索功能正常\")\n",
    "        else:\n",
    "            print(\"⚠️ 搜索结果异常\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 搜索测试失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "Task 3成功完成了以下功能:\n",
    "\n",
    "✅ **数据加载**: 从`vectorized_chunks.jsonl`加载向量和元数据\n",
    "✅ **索引构建**: 使用FAISS IndexFlatL2构建高效向量索引\n",
    "✅ **GPU加速**: 自动检测并使用GPU加速构建过程\n",
    "✅ **数据对齐**: 确保向量索引与元数据映射的严格对应\n",
    "✅ **持久化存储**: 生成可重用的索引和元数据文件\n",
    "✅ **搜索功能**: 支持高效的相似度搜索\n",
    "\n",
    "生成的向量知识库现在可以用于RAG系统的快速检索阶段。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}