{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: 构建与索引向量知识库\n",
    "\n",
    "本Notebook演示如何使用Task 3的功能将Task 2生成的向量化知识块构建成可进行高效相似度搜索的向量知识库。\n",
    "\n",
    "## 主要功能\n",
    "\n",
    "1. **数据加载**: 读取`vectorized_chunks.jsonl`文件\n",
    "2. **索引构建**: 使用FAISS构建高效的向量索引\n",
    "3. **GPU加速**: 自动检测并使用GPU加速索引构建\n",
    "4. **文件保存**: 生成`knowledge_base.index`和`chunk_metadata.pkl`文件\n",
    "\n",
    "## 输出文件\n",
    "\n",
    "- `output/knowledge_base.index`: FAISS二进制索引文件\n",
    "- `output/chunk_metadata.pkl`: 元数据映射文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境检查与依赖导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 检查必要的库\n",
    "try:\n",
    "    import faiss\n",
    "    print(f\"✅ FAISS版本: {faiss.__version__}\")\n",
    "    \n",
    "    # 检查GPU支持\n",
    "    gpu_count = faiss.get_num_gpus()\n",
    "    if gpu_count > 0:\n",
    "        print(f\"🚀 检测到 {gpu_count} 个GPU，将使用GPU加速\")\n",
    "    else:\n",
    "        print(\"💻 未检测到GPU，将使用CPU构建索引\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ 请先安装faiss-gpu: pip install faiss-gpu\")\n",
    "\n",
    "# 检查输入文件\n",
    "input_file = \"output/vectorized_chunks.jsonl\"\n",
    "if os.path.exists(input_file):\n",
    "    file_size = os.path.getsize(input_file) / (1024 * 1024)  # MB\n",
    "    print(f\"✅ 输入文件存在: {input_file} ({file_size:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"❌ 输入文件不存在: {input_file}\")\n",
    "    print(\"请先运行Task 2生成向量化数据文件\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入构建器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_index import VectorKnowledgeBaseBuilder\n",
    "\n",
    "# 创建构建器实例\n",
    "builder = VectorKnowledgeBaseBuilder(\n",
    "    input_file=\"output/vectorized_chunks.jsonl\",\n",
    "    output_dir=\"output\"\n",
    ")\n",
    "\n",
    "print(\"构建器已初始化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据预览\n",
    "\n",
    "在构建索引之前，让我们先查看一下输入数据的结构和统计信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预览输入数据\n",
    "def preview_input_data(file_path, max_lines=5):\n",
    "    \"\"\"预览输入数据文件\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"文件不存在: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"=== 数据预览: {file_path} ===")\n",
    "    \n",
    "    total_lines = 0\n",
    "    vector_dims = set()\n",
    "    source_files = set()\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            total_lines += 1\n",
    "            \n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # 收集统计信息\n",
    "                if 'vector' in data:\n",
    "                    vector_dims.add(len(data['vector']))\n",
    "                if 'metadata' in data and 'source_filename' in data['metadata']:\n",
    "                    source_files.add(data['metadata']['source_filename'])\n",
    "                \n",
    "                # 显示前几行\n",
    "                if i < max_lines:\n",
    "                    print(f\"\\n第 {i+1} 行:\")\n",
    "                    print(f\"  chunk_id: {data.get('chunk_id', 'N/A')}\")\n",
    "                    print(f\"  向量维度: {len(data.get('vector', []))}\")\n",
    "                    print(f\"  内容长度: {len(data.get('content', ''))}\")\n",
    "                    if 'metadata' in data:\n",
    "                        metadata = data['metadata']\n",
    "                        print(f\"  来源文件: {metadata.get('source_filename', 'N/A')}\")\n",
    "                        print(f\"  页码: {metadata.get('page_number', 'N/A')}\")\n",
    "                        \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"第 {i+1} 行JSON解析错误: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== 统计信息 ===")\n",
    "    print(f\"总行数: {total_lines}\")\n",
    "    print(f\"向量维度: {list(vector_dims)}\")\n",
    "    print(f\"来源文件数: {len(source_files)}\")\n",
    "    if source_files:\n",
    "        print(f\"来源文件: {list(source_files)[:5]}{'...' if len(source_files) > 5 else ''}\")\n",
    "\n",
    "# 预览数据\n",
    "preview_input_data(\"output/vectorized_chunks.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 执行向量知识库构建\n",
    "\n",
    "现在开始构建向量知识库。这个过程包括数据加载、FAISS索引构建和文件保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行完整的构建流程\n",
    "try:\n",
    "    print(\"开始构建向量知识库...\")\n",
    "    builder.build()\n",
    "    print(\"\\n🎉 向量知识库构建完成!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 构建失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 验证输出文件\n",
    "\n",
    "检查生成的索引文件和元数据文件是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证输出文件\n",
    "def verify_output_files():\n",
    "    \"\"\"验证输出文件\"\"\"\n",
    "    index_file = \"output/knowledge_base.index\"\n",
    "    metadata_file = \"output/chunk_metadata.pkl\"\n",
    "    \n",
    "    print(\"=== 输出文件验证 ===")\n",
    "    \n",
    "    # 检查文件存在性\n",
    "    if os.path.exists(index_file):\n",
    "        size_mb = os.path.getsize(index_file) / (1024 * 1024)\n",
    "        print(f\"✅ 索引文件: {index_file} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"❌ 索引文件不存在: {index_file}\")\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(metadata_file):\n",
    "        size_mb = os.path.getsize(metadata_file) / (1024 * 1024)\n",
    "        print(f\"✅ 元数据文件: {metadata_file} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"❌ 元数据文件不存在: {metadata_file}\")\n",
    "        return\n",
    "    \n",
    "    # 加载并验证索引\n",
    "    try:\n",
    "        index = faiss.read_index(index_file)\n",
    "        print(f\"📊 索引信息:\")\n",
    "        print(f\"   向量数量: {index.ntotal:,}\")\n",
    "        print(f\"   向量维度: {index.d}\")\n",
    "        print(f\"   索引类型: {type(index).__name__}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 索引文件加载失败: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 加载并验证元数据\n",
    "    try:\n",
    "        with open(metadata_file, 'rb') as f:\n",
    "            metadata_list = pickle.load(f)\n",
    "        \n",
    "        print(f\"📋 元数据信息:\")\n",
    "        print(f\"   元数据条目数: {len(metadata_list):,}\")\n",
    "        \n",
    "        # 验证数据对齐\n",
    "        if len(metadata_list) == index.ntotal:\n",
    "            print(f\"✅ 数据对齐正确: 向量数量 = 元数据数量 = {index.ntotal:,}\")\n",
    "        else:\n",
    "            print(f\"❌ 数据对齐错误: 向量数量({index.ntotal}) != 元数据数量({len(metadata_list)})\")\n",
    "        \n",
    "        # 显示前几个元数据示例\n",
    "        print(f\"\\n📝 元数据示例:\")\n",
    "        for i in range(min(3, len(metadata_list))):\n",
    "            metadata = metadata_list[i]\n",
    "            print(f\"   [{i}] chunk_id: {metadata.get('chunk_id', 'N/A')}\")\n",
    "            if 'metadata' in metadata:\n",
    "                meta = metadata['metadata']\n",
    "                print(f\"       来源: {meta.get('source_filename', 'N/A')}\")\n",
    "                print(f\"       页码: {meta.get('page_number', 'N/A')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 元数据文件加载失败: {e}\")\n",
    "        return\n",
    "    \n",
    "    return index, metadata_list\n",
    "\n",
    "# 执行验证\n",
    "result = verify_output_files()\n",
    "if result:\n",
    "    index, metadata_list = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 测试搜索功能\n",
    "\n",
    "测试构建好的向量知识库的搜索功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试搜索功能\n",
    "def test_search_functionality(index, metadata_list, k=5):\n",
    "    \"\"\"测试搜索功能\"\"\"\n",
    "    if index is None or metadata_list is None:\n",
    "        print(\"索引或元数据未加载，无法测试搜索功能\")\n",
    "        return\n",
    "    \n",
    "    print(f\"=== 搜索功能测试 (Top-{k}) ===")\n",
    "    \n",
    "    # 使用第一个向量作为查询向量\n",
    "    if index.ntotal == 0:\n",
    "        print(\"索引为空，无法进行搜索测试\")\n",
    "        return\n",
    "    \n",
    "    # 重构第一个向量作为查询\n",
    "    query_vector = index.reconstruct(0).reshape(1, -1)\n",
    "    \n",
    "    # 执行搜索\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    print(f\"查询向量维度: {query_vector.shape}\")\n",
    "    print(f\"搜索结果:\")\n",
    "    \n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if idx < len(metadata_list):\n",
    "            metadata = metadata_list[idx]\n",
    "            chunk_id = metadata.get('chunk_id', 'N/A')\n",
    "            source = 'N/A'\n",
    "            page = 'N/A'\n",
    "            \n",
    "            if 'metadata' in metadata:\n",
    "                meta = metadata['metadata']\n",
    "                source = meta.get('source_filename', 'N/A')\n",
    "                page = meta.get('page_number', 'N/A')\n",
    "            \n",
    "            print(f\"  [{i+1}] 距离: {dist:.6f}\")\n",
    "            print(f\"      索引: {idx}\")\n",
    "            print(f\"      chunk_id: {chunk_id}\")\n",
    "            print(f\"      来源: {source}\")\n",
    "            print(f\"      页码: {page}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"  [{i+1}] 索引 {idx} 超出元数据范围\")\n",
    "    \n",
    "    # 验证最相似的应该是自己\n",
    "    if indices[0][0] == 0 and distances[0][0] < 1e-6:\n",
    "        print(\"✅ 搜索功能正常: 最相似的向量是查询向量本身\")\n",
    "    else:\n",
    "        print(f\"⚠️  搜索结果异常: 最相似向量索引={indices[0][0]}, 距离={distances[0][0]}\")\n",
    "\n",
    "# 执行搜索测试\n",
    "if 'index' in locals() and 'metadata_list' in locals():\n",
    "    test_search_functionality(index, metadata_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 性能统计\n",
    "\n",
    "显示构建过程的性能统计信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能统计\n",
    "def show_performance_stats():\n",
    "    \"\"\"显示性能统计\"\"\"\n",
    "    print(\"=== 性能统计 ===")\n",
    "    \n",
    "    # 文件大小统计\n",
    "    files_info = [\n",
    "        (\"输入文件\", \"output/vectorized_chunks.jsonl\"),\n",
    "        (\"索引文件\", \"output/knowledge_base.index\"),\n",
    "        (\"元数据文件\", \"output/chunk_metadata.pkl\")\n",
    "    ]\n",
    "    \n",
    "    total_size = 0\n",
    "    for name, path in files_info:\n",
    "        if os.path.exists(path):\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            total_size += size_mb\n",
    "            print(f\"{name}: {size_mb:.2f} MB\")\n",
    "        else:\n",
    "            print(f\"{name}: 文件不存在\")\n",
    "    \n",
    "    print(f\"总存储空间: {total_size:.2f} MB\")\n",
    "    \n",
    "    # 索引效率\n",
    "    if 'index' in locals():\n",
    "        print(f\"\\n索引效率:\")\n",
    "        print(f\"向量数量: {index.ntotal:,}\")\n",
    "        print(f\"向量维度: {index.d}\")\n",
    "        \n",
    "        # 估算内存使用\n",
    "        vector_memory = index.ntotal * index.d * 4 / (1024 * 1024)  # float32 = 4 bytes\n",
    "        print(f\"向量数据内存: {vector_memory:.2f} MB\")\n",
    "        \n",
    "        # 压缩比\n",
    "        if os.path.exists(\"output/knowledge_base.index\"):\n",
    "            index_size = os.path.getsize(\"output/knowledge_base.index\") / (1024 * 1024)\n",
    "            compression_ratio = vector_memory / index_size if index_size > 0 else 0\n",
    "            print(f\"索引压缩比: {compression_ratio:.2f}x\")\n",
    "\n",
    "# 显示性能统计\n",
    "show_performance_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 使用指南\n",
    "\n",
    "### 命令行使用\n",
    "\n",
    "```bash\n",
    "# 直接运行构建脚本\n",
    "python build_index.py\n",
    "\n",
    "# 运行测试\n",
    "python test_build_index.py\n",
    "```\n",
    "\n",
    "### 程序化使用\n",
    "\n",
    "```python\n",
    "from build_index import VectorKnowledgeBaseBuilder\n",
    "\n",
    "# 创建构建器\n",
    "builder = VectorKnowledgeBaseBuilder(\n",
    "    input_file=\"output/vectorized_chunks.jsonl\",\n",
    "    output_dir=\"output\"\n",
    ")\n",
    "\n",
    "# 执行构建\n",
    "builder.build()\n",
    "```\n",
    "\n",
    "### 加载和使用索引\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# 加载索引\n",
    "index = faiss.read_index(\"output/knowledge_base.index\")\n",
    "\n",
    "# 加载元数据\n",
    "with open(\"output/chunk_metadata.pkl\", \"rb\") as f:\n",
    "    metadata_list = pickle.load(f)\n",
    "\n",
    "# 执行搜索\n",
    "distances, indices = index.search(query_vector, k=5)\n",
    "\n",
    "# 获取结果元数据\n",
    "for idx in indices[0]:\n",
    "    metadata = metadata_list[idx]\n",
    "    print(metadata['chunk_id'], metadata['metadata'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "Task 3成功完成了以下功能:\n",
    "\n",
    "✅ **数据加载**: 从`vectorized_chunks.jsonl`加载向量和元数据\n",
    "✅ **索引构建**: 使用FAISS IndexFlatL2构建高效向量索引\n",
    "✅ **GPU加速**: 自动检测并使用GPU加速构建过程\n",
    "✅ **数据对齐**: 确保向量索引与元数据映射的严格对应\n",
    "✅ **持久化存储**: 生成可重用的索引和元数据文件\n",
    "✅ **错误处理**: 完善的输入验证和错误提示\n",
    "✅ **搜索功能**: 支持高效的相似度搜索\n",
    "\n",
    "生成的向量知识库现在可以用于RAG系统的快速检索阶段。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}