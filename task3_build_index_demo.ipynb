{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: æ„å»ºä¸ç´¢å¼•å‘é‡çŸ¥è¯†åº“\n",
    "\n",
    "æœ¬Notebookæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Task 3çš„åŠŸèƒ½å°†Task 2ç”Ÿæˆçš„å‘é‡åŒ–çŸ¥è¯†å—æ„å»ºæˆå¯è¿›è¡Œé«˜æ•ˆç›¸ä¼¼åº¦æœç´¢çš„å‘é‡çŸ¥è¯†åº“ã€‚\n",
    "\n",
    "## ä¸»è¦åŠŸèƒ½\n",
    "\n",
    "1. **æ•°æ®åŠ è½½**: è¯»å–`vectorized_chunks.jsonl`æ–‡ä»¶\n",
    "2. **ç´¢å¼•æ„å»º**: ä½¿ç”¨FAISSæ„å»ºé«˜æ•ˆçš„å‘é‡ç´¢å¼•\n",
    "3. **GPUåŠ é€Ÿ**: è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨GPUåŠ é€Ÿç´¢å¼•æ„å»º\n",
    "4. **æ–‡ä»¶ä¿å­˜**: ç”Ÿæˆ`knowledge_base.index`å’Œ`chunk_metadata.pkl`æ–‡ä»¶\n",
    "\n",
    "## è¾“å‡ºæ–‡ä»¶\n",
    "\n",
    "- `output/knowledge_base.index`: FAISSäºŒè¿›åˆ¶ç´¢å¼•æ–‡ä»¶\n",
    "- `output/chunk_metadata.pkl`: å…ƒæ•°æ®æ˜ å°„æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒæ£€æŸ¥ä¸ä¾èµ–å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# æ£€æŸ¥å¿…è¦çš„åº“\n",
    "try:\n",
    "    import faiss\n",
    "    print(f\"âœ… FAISSç‰ˆæœ¬: {faiss.__version__}\")\n",
    "    \n",
    "    # æ£€æŸ¥GPUæ”¯æŒ\n",
    "    gpu_count = faiss.get_num_gpus()\n",
    "    if gpu_count > 0:\n",
    "        print(f\"ğŸš€ æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUï¼Œå°†ä½¿ç”¨GPUåŠ é€Ÿ\")\n",
    "    else:\n",
    "        print(\"ğŸ’» æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ„å»ºç´¢å¼•\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âŒ è¯·å…ˆå®‰è£…faiss-gpu: pip install faiss-gpu\")\n",
    "\n",
    "# æ£€æŸ¥è¾“å…¥æ–‡ä»¶\n",
    "input_file = \"output/vectorized_chunks.jsonl\"\n",
    "if os.path.exists(input_file):\n",
    "    file_size = os.path.getsize(input_file) / (1024 * 1024)  # MB\n",
    "    print(f\"âœ… è¾“å…¥æ–‡ä»¶å­˜åœ¨: {input_file} ({file_size:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}\")\n",
    "    print(\"è¯·å…ˆè¿è¡ŒTask 2ç”Ÿæˆå‘é‡åŒ–æ•°æ®æ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å¯¼å…¥æ„å»ºå™¨ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_index import VectorKnowledgeBaseBuilder\n",
    "\n",
    "# åˆ›å»ºæ„å»ºå™¨å®ä¾‹\n",
    "builder = VectorKnowledgeBaseBuilder(\n",
    "    input_file=\"output/vectorized_chunks.jsonl\",\n",
    "    output_dir=\"output\"\n",
    ")\n",
    "\n",
    "print(\"æ„å»ºå™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ•°æ®é¢„è§ˆ\n",
    "\n",
    "åœ¨æ„å»ºç´¢å¼•ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆæŸ¥çœ‹ä¸€ä¸‹è¾“å…¥æ•°æ®çš„ç»“æ„å’Œç»Ÿè®¡ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„è§ˆè¾“å…¥æ•°æ®\n",
    "def preview_input_data(file_path, max_lines=5):\n",
    "    \"\"\"é¢„è§ˆè¾“å…¥æ•°æ®æ–‡ä»¶\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"=== æ•°æ®é¢„è§ˆ: {file_path} ===")\n",
    "    \n",
    "    total_lines = 0\n",
    "    vector_dims = set()\n",
    "    source_files = set()\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            total_lines += 1\n",
    "            \n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯\n",
    "                if 'vector' in data:\n",
    "                    vector_dims.add(len(data['vector']))\n",
    "                if 'metadata' in data and 'source_filename' in data['metadata']:\n",
    "                    source_files.add(data['metadata']['source_filename'])\n",
    "                \n",
    "                # æ˜¾ç¤ºå‰å‡ è¡Œ\n",
    "                if i < max_lines:\n",
    "                    print(f\"\\nç¬¬ {i+1} è¡Œ:\")\n",
    "                    print(f\"  chunk_id: {data.get('chunk_id', 'N/A')}\")\n",
    "                    print(f\"  å‘é‡ç»´åº¦: {len(data.get('vector', []))}\")\n",
    "                    print(f\"  å†…å®¹é•¿åº¦: {len(data.get('content', ''))}\")\n",
    "                    if 'metadata' in data:\n",
    "                        metadata = data['metadata']\n",
    "                        print(f\"  æ¥æºæ–‡ä»¶: {metadata.get('source_filename', 'N/A')}\")\n",
    "                        print(f\"  é¡µç : {metadata.get('page_number', 'N/A')}\")\n",
    "                        \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"ç¬¬ {i+1} è¡ŒJSONè§£æé”™è¯¯: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== ç»Ÿè®¡ä¿¡æ¯ ===")\n",
    "    print(f\"æ€»è¡Œæ•°: {total_lines}\")\n",
    "    print(f\"å‘é‡ç»´åº¦: {list(vector_dims)}\")\n",
    "    print(f\"æ¥æºæ–‡ä»¶æ•°: {len(source_files)}\")\n",
    "    if source_files:\n",
    "        print(f\"æ¥æºæ–‡ä»¶: {list(source_files)[:5]}{'...' if len(source_files) > 5 else ''}\")\n",
    "\n",
    "# é¢„è§ˆæ•°æ®\n",
    "preview_input_data(\"output/vectorized_chunks.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ‰§è¡Œå‘é‡çŸ¥è¯†åº“æ„å»º\n",
    "\n",
    "ç°åœ¨å¼€å§‹æ„å»ºå‘é‡çŸ¥è¯†åº“ã€‚è¿™ä¸ªè¿‡ç¨‹åŒ…æ‹¬æ•°æ®åŠ è½½ã€FAISSç´¢å¼•æ„å»ºå’Œæ–‡ä»¶ä¿å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰§è¡Œå®Œæ•´çš„æ„å»ºæµç¨‹\n",
    "try:\n",
    "    print(\"å¼€å§‹æ„å»ºå‘é‡çŸ¥è¯†åº“...\")\n",
    "    builder.build()\n",
    "    print(\"\\nğŸ‰ å‘é‡çŸ¥è¯†åº“æ„å»ºå®Œæˆ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ„å»ºå¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. éªŒè¯è¾“å‡ºæ–‡ä»¶\n",
    "\n",
    "æ£€æŸ¥ç”Ÿæˆçš„ç´¢å¼•æ–‡ä»¶å’Œå…ƒæ•°æ®æ–‡ä»¶æ˜¯å¦æ­£ç¡®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯è¾“å‡ºæ–‡ä»¶\n",
    "def verify_output_files():\n",
    "    \"\"\"éªŒè¯è¾“å‡ºæ–‡ä»¶\"\"\"\n",
    "    index_file = \"output/knowledge_base.index\"\n",
    "    metadata_file = \"output/chunk_metadata.pkl\"\n",
    "    \n",
    "    print(\"=== è¾“å‡ºæ–‡ä»¶éªŒè¯ ===")\n",
    "    \n",
    "    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§\n",
    "    if os.path.exists(index_file):\n",
    "        size_mb = os.path.getsize(index_file) / (1024 * 1024)\n",
    "        print(f\"âœ… ç´¢å¼•æ–‡ä»¶: {index_file} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file}\")\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(metadata_file):\n",
    "        size_mb = os.path.getsize(metadata_file) / (1024 * 1024)\n",
    "        print(f\"âœ… å…ƒæ•°æ®æ–‡ä»¶: {metadata_file} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}\")\n",
    "        return\n",
    "    \n",
    "    # åŠ è½½å¹¶éªŒè¯ç´¢å¼•\n",
    "    try:\n",
    "        index = faiss.read_index(index_file)\n",
    "        print(f\"ğŸ“Š ç´¢å¼•ä¿¡æ¯:\")\n",
    "        print(f\"   å‘é‡æ•°é‡: {index.ntotal:,}\")\n",
    "        print(f\"   å‘é‡ç»´åº¦: {index.d}\")\n",
    "        print(f\"   ç´¢å¼•ç±»å‹: {type(index).__name__}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç´¢å¼•æ–‡ä»¶åŠ è½½å¤±è´¥: {e}\")\n",
    "        return\n",
    "    \n",
    "    # åŠ è½½å¹¶éªŒè¯å…ƒæ•°æ®\n",
    "    try:\n",
    "        with open(metadata_file, 'rb') as f:\n",
    "            metadata_list = pickle.load(f)\n",
    "        \n",
    "        print(f\"ğŸ“‹ å…ƒæ•°æ®ä¿¡æ¯:\")\n",
    "        print(f\"   å…ƒæ•°æ®æ¡ç›®æ•°: {len(metadata_list):,}\")\n",
    "        \n",
    "        # éªŒè¯æ•°æ®å¯¹é½\n",
    "        if len(metadata_list) == index.ntotal:\n",
    "            print(f\"âœ… æ•°æ®å¯¹é½æ­£ç¡®: å‘é‡æ•°é‡ = å…ƒæ•°æ®æ•°é‡ = {index.ntotal:,}\")\n",
    "        else:\n",
    "            print(f\"âŒ æ•°æ®å¯¹é½é”™è¯¯: å‘é‡æ•°é‡({index.ntotal}) != å…ƒæ•°æ®æ•°é‡({len(metadata_list)})\")\n",
    "        \n",
    "        # æ˜¾ç¤ºå‰å‡ ä¸ªå…ƒæ•°æ®ç¤ºä¾‹\n",
    "        print(f\"\\nğŸ“ å…ƒæ•°æ®ç¤ºä¾‹:\")\n",
    "        for i in range(min(3, len(metadata_list))):\n",
    "            metadata = metadata_list[i]\n",
    "            print(f\"   [{i}] chunk_id: {metadata.get('chunk_id', 'N/A')}\")\n",
    "            if 'metadata' in metadata:\n",
    "                meta = metadata['metadata']\n",
    "                print(f\"       æ¥æº: {meta.get('source_filename', 'N/A')}\")\n",
    "                print(f\"       é¡µç : {meta.get('page_number', 'N/A')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å…ƒæ•°æ®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}\")\n",
    "        return\n",
    "    \n",
    "    return index, metadata_list\n",
    "\n",
    "# æ‰§è¡ŒéªŒè¯\n",
    "result = verify_output_files()\n",
    "if result:\n",
    "    index, metadata_list = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æµ‹è¯•æœç´¢åŠŸèƒ½\n",
    "\n",
    "æµ‹è¯•æ„å»ºå¥½çš„å‘é‡çŸ¥è¯†åº“çš„æœç´¢åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æœç´¢åŠŸèƒ½\n",
    "def test_search_functionality(index, metadata_list, k=5):\n",
    "    \"\"\"æµ‹è¯•æœç´¢åŠŸèƒ½\"\"\"\n",
    "    if index is None or metadata_list is None:\n",
    "        print(\"ç´¢å¼•æˆ–å…ƒæ•°æ®æœªåŠ è½½ï¼Œæ— æ³•æµ‹è¯•æœç´¢åŠŸèƒ½\")\n",
    "        return\n",
    "    \n",
    "    print(f\"=== æœç´¢åŠŸèƒ½æµ‹è¯• (Top-{k}) ===")\n",
    "    \n",
    "    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå‘é‡ä½œä¸ºæŸ¥è¯¢å‘é‡\n",
    "    if index.ntotal == 0:\n",
    "        print(\"ç´¢å¼•ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæœç´¢æµ‹è¯•\")\n",
    "        return\n",
    "    \n",
    "    # é‡æ„ç¬¬ä¸€ä¸ªå‘é‡ä½œä¸ºæŸ¥è¯¢\n",
    "    query_vector = index.reconstruct(0).reshape(1, -1)\n",
    "    \n",
    "    # æ‰§è¡Œæœç´¢\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    print(f\"æŸ¥è¯¢å‘é‡ç»´åº¦: {query_vector.shape}\")\n",
    "    print(f\"æœç´¢ç»“æœ:\")\n",
    "    \n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if idx < len(metadata_list):\n",
    "            metadata = metadata_list[idx]\n",
    "            chunk_id = metadata.get('chunk_id', 'N/A')\n",
    "            source = 'N/A'\n",
    "            page = 'N/A'\n",
    "            \n",
    "            if 'metadata' in metadata:\n",
    "                meta = metadata['metadata']\n",
    "                source = meta.get('source_filename', 'N/A')\n",
    "                page = meta.get('page_number', 'N/A')\n",
    "            \n",
    "            print(f\"  [{i+1}] è·ç¦»: {dist:.6f}\")\n",
    "            print(f\"      ç´¢å¼•: {idx}\")\n",
    "            print(f\"      chunk_id: {chunk_id}\")\n",
    "            print(f\"      æ¥æº: {source}\")\n",
    "            print(f\"      é¡µç : {page}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"  [{i+1}] ç´¢å¼• {idx} è¶…å‡ºå…ƒæ•°æ®èŒƒå›´\")\n",
    "    \n",
    "    # éªŒè¯æœ€ç›¸ä¼¼çš„åº”è¯¥æ˜¯è‡ªå·±\n",
    "    if indices[0][0] == 0 and distances[0][0] < 1e-6:\n",
    "        print(\"âœ… æœç´¢åŠŸèƒ½æ­£å¸¸: æœ€ç›¸ä¼¼çš„å‘é‡æ˜¯æŸ¥è¯¢å‘é‡æœ¬èº«\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  æœç´¢ç»“æœå¼‚å¸¸: æœ€ç›¸ä¼¼å‘é‡ç´¢å¼•={indices[0][0]}, è·ç¦»={distances[0][0]}\")\n",
    "\n",
    "# æ‰§è¡Œæœç´¢æµ‹è¯•\n",
    "if 'index' in locals() and 'metadata_list' in locals():\n",
    "    test_search_functionality(index, metadata_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ€§èƒ½ç»Ÿè®¡\n",
    "\n",
    "æ˜¾ç¤ºæ„å»ºè¿‡ç¨‹çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½ç»Ÿè®¡\n",
    "def show_performance_stats():\n",
    "    \"\"\"æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡\"\"\"\n",
    "    print(\"=== æ€§èƒ½ç»Ÿè®¡ ===")\n",
    "    \n",
    "    # æ–‡ä»¶å¤§å°ç»Ÿè®¡\n",
    "    files_info = [\n",
    "        (\"è¾“å…¥æ–‡ä»¶\", \"output/vectorized_chunks.jsonl\"),\n",
    "        (\"ç´¢å¼•æ–‡ä»¶\", \"output/knowledge_base.index\"),\n",
    "        (\"å…ƒæ•°æ®æ–‡ä»¶\", \"output/chunk_metadata.pkl\")\n",
    "    ]\n",
    "    \n",
    "    total_size = 0\n",
    "    for name, path in files_info:\n",
    "        if os.path.exists(path):\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            total_size += size_mb\n",
    "            print(f\"{name}: {size_mb:.2f} MB\")\n",
    "        else:\n",
    "            print(f\"{name}: æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "    \n",
    "    print(f\"æ€»å­˜å‚¨ç©ºé—´: {total_size:.2f} MB\")\n",
    "    \n",
    "    # ç´¢å¼•æ•ˆç‡\n",
    "    if 'index' in locals():\n",
    "        print(f\"\\nç´¢å¼•æ•ˆç‡:\")\n",
    "        print(f\"å‘é‡æ•°é‡: {index.ntotal:,}\")\n",
    "        print(f\"å‘é‡ç»´åº¦: {index.d}\")\n",
    "        \n",
    "        # ä¼°ç®—å†…å­˜ä½¿ç”¨\n",
    "        vector_memory = index.ntotal * index.d * 4 / (1024 * 1024)  # float32 = 4 bytes\n",
    "        print(f\"å‘é‡æ•°æ®å†…å­˜: {vector_memory:.2f} MB\")\n",
    "        \n",
    "        # å‹ç¼©æ¯”\n",
    "        if os.path.exists(\"output/knowledge_base.index\"):\n",
    "            index_size = os.path.getsize(\"output/knowledge_base.index\") / (1024 * 1024)\n",
    "            compression_ratio = vector_memory / index_size if index_size > 0 else 0\n",
    "            print(f\"ç´¢å¼•å‹ç¼©æ¯”: {compression_ratio:.2f}x\")\n",
    "\n",
    "# æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡\n",
    "show_performance_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ä½¿ç”¨æŒ‡å—\n",
    "\n",
    "### å‘½ä»¤è¡Œä½¿ç”¨\n",
    "\n",
    "```bash\n",
    "# ç›´æ¥è¿è¡Œæ„å»ºè„šæœ¬\n",
    "python build_index.py\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "python test_build_index.py\n",
    "```\n",
    "\n",
    "### ç¨‹åºåŒ–ä½¿ç”¨\n",
    "\n",
    "```python\n",
    "from build_index import VectorKnowledgeBaseBuilder\n",
    "\n",
    "# åˆ›å»ºæ„å»ºå™¨\n",
    "builder = VectorKnowledgeBaseBuilder(\n",
    "    input_file=\"output/vectorized_chunks.jsonl\",\n",
    "    output_dir=\"output\"\n",
    ")\n",
    "\n",
    "# æ‰§è¡Œæ„å»º\n",
    "builder.build()\n",
    "```\n",
    "\n",
    "### åŠ è½½å’Œä½¿ç”¨ç´¢å¼•\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# åŠ è½½ç´¢å¼•\n",
    "index = faiss.read_index(\"output/knowledge_base.index\")\n",
    "\n",
    "# åŠ è½½å…ƒæ•°æ®\n",
    "with open(\"output/chunk_metadata.pkl\", \"rb\") as f:\n",
    "    metadata_list = pickle.load(f)\n",
    "\n",
    "# æ‰§è¡Œæœç´¢\n",
    "distances, indices = index.search(query_vector, k=5)\n",
    "\n",
    "# è·å–ç»“æœå…ƒæ•°æ®\n",
    "for idx in indices[0]:\n",
    "    metadata = metadata_list[idx]\n",
    "    print(metadata['chunk_id'], metadata['metadata'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "Task 3æˆåŠŸå®Œæˆäº†ä»¥ä¸‹åŠŸèƒ½:\n",
    "\n",
    "âœ… **æ•°æ®åŠ è½½**: ä»`vectorized_chunks.jsonl`åŠ è½½å‘é‡å’Œå…ƒæ•°æ®\n",
    "âœ… **ç´¢å¼•æ„å»º**: ä½¿ç”¨FAISS IndexFlatL2æ„å»ºé«˜æ•ˆå‘é‡ç´¢å¼•\n",
    "âœ… **GPUåŠ é€Ÿ**: è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨GPUåŠ é€Ÿæ„å»ºè¿‡ç¨‹\n",
    "âœ… **æ•°æ®å¯¹é½**: ç¡®ä¿å‘é‡ç´¢å¼•ä¸å…ƒæ•°æ®æ˜ å°„çš„ä¸¥æ ¼å¯¹åº”\n",
    "âœ… **æŒä¹…åŒ–å­˜å‚¨**: ç”Ÿæˆå¯é‡ç”¨çš„ç´¢å¼•å’Œå…ƒæ•°æ®æ–‡ä»¶\n",
    "âœ… **é”™è¯¯å¤„ç†**: å®Œå–„çš„è¾“å…¥éªŒè¯å’Œé”™è¯¯æç¤º\n",
    "âœ… **æœç´¢åŠŸèƒ½**: æ”¯æŒé«˜æ•ˆçš„ç›¸ä¼¼åº¦æœç´¢\n",
    "\n",
    "ç”Ÿæˆçš„å‘é‡çŸ¥è¯†åº“ç°åœ¨å¯ä»¥ç”¨äºRAGç³»ç»Ÿçš„å¿«é€Ÿæ£€ç´¢é˜¶æ®µã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}