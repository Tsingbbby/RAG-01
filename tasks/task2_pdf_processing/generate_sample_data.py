#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿæˆç¤ºä¾‹æ•°æ®å’Œå‘é‡åŒ–æ–‡ä»¶
ç”¨äºæ¼”ç¤ºRAGç®¡é“åŠŸèƒ½
"""

import os
import json
import numpy as np
from typing import List, Dict, Any
from embedding_module import EmbeddingAPIClient

class SampleDataGenerator:
    """ç¤ºä¾‹æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.embedding_client = EmbeddingAPIClient()
        self.output_dir = "output"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
    
    def generate_sample_documents(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç¤ºä¾‹æ–‡æ¡£æ•°æ®"""
        documents = [
            {
                "id": "doc_001",
                "title": "äººå·¥æ™ºèƒ½åŸºç¡€",
                "content": "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚äººå·¥æ™ºèƒ½çš„ç ”ç©¶é¢†åŸŸåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰ã€‚",
                "source": "AIæ•™ç¨‹ç¬¬1ç« ",
                "metadata": {"category": "åŸºç¡€æ¦‚å¿µ", "difficulty": "åˆçº§"}
            },
            {
                "id": "doc_002",
                "title": "æœºå™¨å­¦ä¹ æ¦‚è¿°",
                "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡è®­ç»ƒæ•°æ®æ¥æ„å»ºæ•°å­¦æ¨¡å‹ï¼Œä»¥ä¾¿å¯¹æ–°æ•°æ®åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚ä¸»è¦çš„æœºå™¨å­¦ä¹ æ–¹æ³•åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚",
                "source": "æœºå™¨å­¦ä¹ å…¥é—¨æŒ‡å—",
                "metadata": {"category": "æœºå™¨å­¦ä¹ ", "difficulty": "ä¸­çº§"}
            },
            {
                "id": "doc_003",
                "title": "æ·±åº¦å­¦ä¹ åŸç†",
                "content": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯æ·±å±‚ç¥ç»ç½‘ç»œã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å¸¸è§çš„æ·±åº¦å­¦ä¹ æ¶æ„åŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’ŒTransformerã€‚",
                "source": "æ·±åº¦å­¦ä¹ å®æˆ˜",
                "metadata": {"category": "æ·±åº¦å­¦ä¹ ", "difficulty": "é«˜çº§"}
            },
            {
                "id": "doc_004",
                "title": "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
                "content": "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦é¢†åŸŸçš„åˆ†æ”¯å­¦ç§‘ã€‚å®ƒç ”ç©¶èƒ½å®ç°äººä¸è®¡ç®—æœºä¹‹é—´ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œæœ‰æ•ˆé€šä¿¡çš„å„ç§ç†è®ºå’Œæ–¹æ³•ã€‚NLPçš„ä¸»è¦ä»»åŠ¡åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚",
                "source": "NLPæŠ€æœ¯æ‰‹å†Œ",
                "metadata": {"category": "è‡ªç„¶è¯­è¨€å¤„ç†", "difficulty": "ä¸­çº§"}
            },
            {
                "id": "doc_005",
                "title": "è®¡ç®—æœºè§†è§‰åº”ç”¨",
                "content": "è®¡ç®—æœºè§†è§‰æ˜¯ä¸€é—¨ç ”ç©¶å¦‚ä½•ä½¿æœºå™¨"çœ‹"çš„ç§‘å­¦ï¼Œæ›´è¿›ä¸€æ­¥çš„è¯´ï¼Œå°±æ˜¯æŒ‡ç”¨æ‘„å½±æœºå’Œç”µè„‘ä»£æ›¿äººçœ¼å¯¹ç›®æ ‡è¿›è¡Œè¯†åˆ«ã€è·Ÿè¸ªå’Œæµ‹é‡ç­‰æœºå™¨è§†è§‰ï¼Œå¹¶è¿›ä¸€æ­¥åšå›¾å½¢å¤„ç†ã€‚è®¡ç®—æœºè§†è§‰çš„åº”ç”¨é¢†åŸŸåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€äººè„¸è¯†åˆ«ã€åŒ»å­¦å½±åƒåˆ†æç­‰ã€‚",
                "source": "è®¡ç®—æœºè§†è§‰å¯¼è®º",
                "metadata": {"category": "è®¡ç®—æœºè§†è§‰", "difficulty": "ä¸­çº§"}
            },
            {
                "id": "doc_006",
                "title": "å¼ºåŒ–å­¦ä¹ åŸºç¡€",
                "content": "å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé¢†åŸŸï¼Œå¼ºè°ƒå¦‚ä½•åŸºäºç¯å¢ƒè€Œè¡ŒåŠ¨ï¼Œä»¥å–å¾—æœ€å¤§åŒ–çš„é¢„æœŸåˆ©ç›Šã€‚å¼ºåŒ–å­¦ä¹ çš„ç‰¹ç‚¹æ˜¯è¯•é”™æœç´¢å’Œå»¶è¿Ÿå›æŠ¥ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚è‘—åçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åŒ…æ‹¬Q-learningã€ç­–ç•¥æ¢¯åº¦æ–¹æ³•å’ŒActor-Criticæ–¹æ³•ã€‚",
                "source": "å¼ºåŒ–å­¦ä¹ åŸç†ä¸å®è·µ",
                "metadata": {"category": "å¼ºåŒ–å­¦ä¹ ", "difficulty": "é«˜çº§"}
            },
            {
                "id": "doc_007",
                "title": "æ•°æ®é¢„å¤„ç†æŠ€æœ¯",
                "content": "æ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ å’Œæ•°æ®æŒ–æ˜è¿‡ç¨‹ä¸­çš„é‡è¦æ­¥éª¤ã€‚å®ƒåŒ…æ‹¬æ•°æ®æ¸…æ´—ã€æ•°æ®é›†æˆã€æ•°æ®å˜æ¢å’Œæ•°æ®è§„çº¦ç­‰æ“ä½œã€‚è‰¯å¥½çš„æ•°æ®é¢„å¤„ç†èƒ½å¤Ÿæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚å¸¸è§çš„é¢„å¤„ç†æŠ€æœ¯åŒ…æ‹¬ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹ã€ç‰¹å¾ç¼©æ”¾ã€ç‰¹å¾é€‰æ‹©ç­‰ã€‚",
                "source": "æ•°æ®ç§‘å­¦å®æˆ˜",
                "metadata": {"category": "æ•°æ®å¤„ç†", "difficulty": "åˆçº§"}
            },
            {
                "id": "doc_008",
                "title": "æ¨¡å‹è¯„ä¼°æ–¹æ³•",
                "content": "æ¨¡å‹è¯„ä¼°æ˜¯æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­çš„å…³é”®ç¯èŠ‚ï¼Œç”¨äºè¡¡é‡æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚å¸¸ç”¨çš„è¯„ä¼°æ–¹æ³•åŒ…æ‹¬äº¤å‰éªŒè¯ã€ç•™å‡ºæ³•ã€è‡ªåŠ©æ³•ç­‰ã€‚è¯„ä¼°æŒ‡æ ‡æ ¹æ®ä»»åŠ¡ç±»å‹è€Œä¸åŒï¼Œåˆ†ç±»ä»»åŠ¡å¸¸ç”¨å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ï¼Œå›å½’ä»»åŠ¡å¸¸ç”¨å‡æ–¹è¯¯å·®ã€å¹³å‡ç»å¯¹è¯¯å·®ç­‰ã€‚",
                "source": "æœºå™¨å­¦ä¹ è¯„ä¼°æŒ‡å—",
                "metadata": {"category": "æ¨¡å‹è¯„ä¼°", "difficulty": "ä¸­çº§"}
            }
        ]
        
        return documents
    
    def chunk_documents(self, documents: List[Dict[str, Any]], chunk_size: int = 200) -> List[Dict[str, Any]]:
        """å°†æ–‡æ¡£åˆ†å—"""
        chunks = []
        
        for doc in documents:
            content = doc["content"]
            
            # ç®€å•çš„åˆ†å—ç­–ç•¥ï¼šæŒ‰å­—ç¬¦é•¿åº¦åˆ†å—
            for i in range(0, len(content), chunk_size):
                chunk_content = content[i:i + chunk_size]
                
                chunk = {
                    "chunk_id": f"{doc['id']}_chunk_{i//chunk_size + 1}",
                    "document_id": doc["id"],
                    "title": doc["title"],
                    "content": chunk_content,
                    "source": doc["source"],
                    "metadata": doc["metadata"].copy(),
                    "chunk_index": i // chunk_size + 1,
                    "start_char": i,
                    "end_char": min(i + chunk_size, len(content))
                }
                
                chunks.append(chunk)
        
        return chunks
    
    def generate_embeddings(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¸ºæ–‡æ¡£å—ç”Ÿæˆå‘é‡åµŒå…¥"""
        vectorized_chunks = []
        
        print(f"æ­£åœ¨ä¸º {len(chunks)} ä¸ªæ–‡æ¡£å—ç”Ÿæˆå‘é‡åµŒå…¥...")
        
        for i, chunk in enumerate(chunks):
            try:
                # ç”Ÿæˆå‘é‡åµŒå…¥
                embedding = self.embedding_client.get_embedding(chunk["content"])
                
                # åˆ›å»ºå‘é‡åŒ–æ•°æ®
                vectorized_chunk = {
                    "chunk_id": chunk["chunk_id"],
                    "document_id": chunk["document_id"],
                    "title": chunk["title"],
                    "content": chunk["content"],
                    "source": chunk["source"],
                    "metadata": chunk["metadata"],
                    "chunk_index": chunk["chunk_index"],
                    "start_char": chunk["start_char"],
                    "end_char": chunk["end_char"],
                    "embedding": embedding,
                    "embedding_dim": len(embedding)
                }
                
                vectorized_chunks.append(vectorized_chunk)
                
                print(f"âœ“ å·²å¤„ç† {i+1}/{len(chunks)} ä¸ªå—")
                
            except Exception as e:
                print(f"âŒ å¤„ç†å— {chunk['chunk_id']} æ—¶å‡ºé”™: {e}")
                continue
        
        return vectorized_chunks
    
    def save_vectorized_data(self, vectorized_chunks: List[Dict[str, Any]]):
        """ä¿å­˜å‘é‡åŒ–æ•°æ®åˆ°æ–‡ä»¶"""
        output_file = os.path.join(self.output_dir, "vectorized_chunks.jsonl")
        
        print(f"æ­£åœ¨ä¿å­˜å‘é‡åŒ–æ•°æ®åˆ° {output_file}...")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for chunk in vectorized_chunks:
                json.dump(chunk, f, ensure_ascii=False)
                f.write('\n')
        
        print(f"âœ… å·²ä¿å­˜ {len(vectorized_chunks)} ä¸ªå‘é‡åŒ–å—åˆ° {output_file}")
    
    def generate_all(self):
        """ç”Ÿæˆå®Œæ•´çš„ç¤ºä¾‹æ•°æ®é›†"""
        print("=== ç”Ÿæˆç¤ºä¾‹æ•°æ®å’Œå‘é‡åŒ–æ–‡ä»¶ ===")
        
        # 1. ç”Ÿæˆç¤ºä¾‹æ–‡æ¡£
        print("\n1. ç”Ÿæˆç¤ºä¾‹æ–‡æ¡£...")
        documents = self.generate_sample_documents()
        print(f"âœ“ ç”Ÿæˆäº† {len(documents)} ä¸ªç¤ºä¾‹æ–‡æ¡£")
        
        # 2. æ–‡æ¡£åˆ†å—
        print("\n2. æ–‡æ¡£åˆ†å—å¤„ç†...")
        chunks = self.chunk_documents(documents, chunk_size=150)
        print(f"âœ“ ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æ¡£å—")
        
        # 3. ç”Ÿæˆå‘é‡åµŒå…¥
        print("\n3. ç”Ÿæˆå‘é‡åµŒå…¥...")
        vectorized_chunks = self.generate_embeddings(chunks)
        print(f"âœ“ æˆåŠŸç”Ÿæˆ {len(vectorized_chunks)} ä¸ªå‘é‡åŒ–å—")
        
        # 4. ä¿å­˜æ•°æ®
        print("\n4. ä¿å­˜å‘é‡åŒ–æ•°æ®...")
        self.save_vectorized_data(vectorized_chunks)
        
        print("\nğŸ‰ ç¤ºä¾‹æ•°æ®ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {os.path.join(self.output_dir, 'vectorized_chunks.jsonl')}")
        
        return vectorized_chunks

def main():
    """ä¸»å‡½æ•°"""
    try:
        generator = SampleDataGenerator()
        generator.generate_all()
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise

if __name__ == "__main__":
    main()