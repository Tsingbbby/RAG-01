#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Task 3 æµ‹è¯•è„šæœ¬: éªŒè¯å‘é‡çŸ¥è¯†åº“æ„å»ºåŠŸèƒ½

æ­¤è„šæœ¬ç”¨äºæµ‹è¯•build_index.pyçš„å„é¡¹åŠŸèƒ½ï¼ŒåŒ…æ‹¬:
1. æ•°æ®åŠ è½½åŠŸèƒ½æµ‹è¯•
2. FAISSç´¢å¼•æ„å»ºæµ‹è¯•
3. æ–‡ä»¶ä¿å­˜å’ŒåŠ è½½æµ‹è¯•
4. é”™è¯¯å¤„ç†æµ‹è¯•
"""

import json
import pickle
import numpy as np
import faiss
import os
import tempfile
import shutil
from typing import List, Dict, Any
from build_index import VectorKnowledgeBaseBuilder


class TestVectorKnowledgeBaseBuilder:
    """å‘é‡çŸ¥è¯†åº“æ„å»ºå™¨æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_dir = None
        self.test_input_file = None
        
    def setup_test_environment(self) -> None:
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
        self.test_dir = tempfile.mkdtemp(prefix="test_build_index_")
        self.test_input_file = os.path.join(self.test_dir, "test_vectorized_chunks.jsonl")
        print(f"æµ‹è¯•ç›®å½•: {self.test_dir}")
    
    def cleanup_test_environment(self) -> None:
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.test_dir and os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)
            print(f"å·²æ¸…ç†æµ‹è¯•ç›®å½•: {self.test_dir}")
    
    def create_test_data(self, num_vectors: int = 10, vector_dim: int = 384) -> None:
        """åˆ›å»ºæµ‹è¯•æ•°æ®"""
        print(f"åˆ›å»ºæµ‹è¯•æ•°æ®: {num_vectors} ä¸ªå‘é‡ï¼Œç»´åº¦ {vector_dim}")
        
        test_data = []
        for i in range(num_vectors):
            # ç”Ÿæˆéšæœºå‘é‡
            vector = np.random.rand(vector_dim).astype(np.float32).tolist()
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®é¡¹
            data_item = {
                "chunk_id": f"test_chunk_{i:03d}",
                "content": f"è¿™æ˜¯æµ‹è¯•å†…å®¹å— {i}",
                "vector": vector,
                "metadata": {
                    "source_filename": f"test_doc_{i // 3}.pdf",
                    "page_number": i % 5 + 1,
                    "chunk_index": i,
                    "content_type": "text"
                }
            }
            test_data.append(data_item)
        
        # å†™å…¥JSONLæ–‡ä»¶
        with open(self.test_input_file, 'w', encoding='utf-8') as f:
            for item in test_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ°: {self.test_input_file}")
    
    def test_data_loading(self) -> None:
        """æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½"""
        print("\n=== æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½ ===")
        
        builder = VectorKnowledgeBaseBuilder(
            input_file=self.test_input_file,
            output_dir=self.test_dir
        )
        
        try:
            vector_array, metadata_list = builder.load_vectorized_data()
            
            # éªŒè¯æ•°æ®
            assert vector_array.shape[0] == 10, f"å‘é‡æ•°é‡é”™è¯¯: {vector_array.shape[0]}"
            assert vector_array.shape[1] == 384, f"å‘é‡ç»´åº¦é”™è¯¯: {vector_array.shape[1]}"
            assert len(metadata_list) == 10, f"å…ƒæ•°æ®æ•°é‡é”™è¯¯: {len(metadata_list)}"
            assert vector_array.dtype == np.float32, f"å‘é‡ç±»å‹é”™è¯¯: {vector_array.dtype}"
            
            # éªŒè¯å…ƒæ•°æ®ç»“æ„
            for i, metadata in enumerate(metadata_list):
                assert 'chunk_id' in metadata, "å…ƒæ•°æ®ç¼ºå°‘chunk_id"
                assert 'metadata' in metadata, "å…ƒæ•°æ®ç¼ºå°‘metadataå­—æ®µ"
                assert metadata['chunk_id'] == f"test_chunk_{i:03d}", "chunk_idä¸åŒ¹é…"
            
            print("âœ… æ•°æ®åŠ è½½æµ‹è¯•é€šè¿‡")
            return vector_array, metadata_list
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def test_faiss_index_building(self, vector_array: np.ndarray) -> faiss.Index:
        """æµ‹è¯•FAISSç´¢å¼•æ„å»º"""
        print("\n=== æµ‹è¯•FAISSç´¢å¼•æ„å»º ===")
        
        builder = VectorKnowledgeBaseBuilder(
            input_file=self.test_input_file,
            output_dir=self.test_dir
        )
        
        try:
            index = builder.build_faiss_index(vector_array)
            
            # éªŒè¯ç´¢å¼•
            assert index.ntotal == 10, f"ç´¢å¼•å‘é‡æ•°é‡é”™è¯¯: {index.ntotal}"
            assert index.d == 384, f"ç´¢å¼•ç»´åº¦é”™è¯¯: {index.d}"
            
            # æµ‹è¯•æœç´¢åŠŸèƒ½
            query_vector = vector_array[0:1]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå‘é‡ä½œä¸ºæŸ¥è¯¢
            distances, indices = index.search(query_vector, k=3)
            
            assert len(distances[0]) == 3, "æœç´¢ç»“æœæ•°é‡é”™è¯¯"
            assert indices[0][0] == 0, "æœ€ç›¸ä¼¼å‘é‡åº”è¯¥æ˜¯è‡ªå·±"
            assert distances[0][0] < 1e-6, "è‡ªç›¸ä¼¼è·ç¦»åº”è¯¥æ¥è¿‘0"
            
            print("âœ… FAISSç´¢å¼•æ„å»ºæµ‹è¯•é€šè¿‡")
            return index
            
        except Exception as e:
            print(f"âŒ FAISSç´¢å¼•æ„å»ºæµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def test_file_saving_and_loading(self, index: faiss.Index, metadata_list: List[Dict[str, Any]]) -> None:
        """æµ‹è¯•æ–‡ä»¶ä¿å­˜å’ŒåŠ è½½"""
        print("\n=== æµ‹è¯•æ–‡ä»¶ä¿å­˜å’ŒåŠ è½½ ===")
        
        builder = VectorKnowledgeBaseBuilder(
            input_file=self.test_input_file,
            output_dir=self.test_dir
        )
        
        try:
            # ä¿å­˜æ–‡ä»¶
            builder.save_outputs(index, metadata_list)
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            assert os.path.exists(builder.index_file), "ç´¢å¼•æ–‡ä»¶æœªåˆ›å»º"
            assert os.path.exists(builder.metadata_file), "å…ƒæ•°æ®æ–‡ä»¶æœªåˆ›å»º"
            
            # æµ‹è¯•åŠ è½½ç´¢å¼•
            loaded_index = faiss.read_index(builder.index_file)
            assert loaded_index.ntotal == index.ntotal, "åŠ è½½çš„ç´¢å¼•å‘é‡æ•°é‡ä¸åŒ¹é…"
            assert loaded_index.d == index.d, "åŠ è½½çš„ç´¢å¼•ç»´åº¦ä¸åŒ¹é…"
            
            # æµ‹è¯•åŠ è½½å…ƒæ•°æ®
            with open(builder.metadata_file, 'rb') as f:
                loaded_metadata = pickle.load(f)
            
            assert len(loaded_metadata) == len(metadata_list), "åŠ è½½çš„å…ƒæ•°æ®æ•°é‡ä¸åŒ¹é…"
            
            # éªŒè¯å…ƒæ•°æ®å†…å®¹
            for i, (original, loaded) in enumerate(zip(metadata_list, loaded_metadata)):
                assert original['chunk_id'] == loaded['chunk_id'], f"ç¬¬{i}ä¸ªchunk_idä¸åŒ¹é…"
                assert original['metadata'] == loaded['metadata'], f"ç¬¬{i}ä¸ªmetadataä¸åŒ¹é…"
            
            print("âœ… æ–‡ä»¶ä¿å­˜å’ŒåŠ è½½æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ä¿å­˜å’ŒåŠ è½½æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def test_error_handling(self) -> None:
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        print("\n=== æµ‹è¯•é”™è¯¯å¤„ç† ===")
        
        # æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨çš„æƒ…å†µ
        try:
            builder = VectorKnowledgeBaseBuilder(
                input_file="nonexistent_file.jsonl",
                output_dir=self.test_dir
            )
            builder.load_vectorized_data()
            assert False, "åº”è¯¥æŠ›å‡ºFileNotFoundError"
        except FileNotFoundError:
            print("âœ… æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯å¤„ç†æ­£ç¡®")
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯å¤„ç†å¤±è´¥: {e}")
            raise
        
        # æµ‹è¯•ç©ºæ–‡ä»¶çš„æƒ…å†µ
        try:
            empty_file = os.path.join(self.test_dir, "empty.jsonl")
            with open(empty_file, 'w') as f:
                pass  # åˆ›å»ºç©ºæ–‡ä»¶
            
            builder = VectorKnowledgeBaseBuilder(
                input_file=empty_file,
                output_dir=self.test_dir
            )
            builder.load_vectorized_data()
            assert False, "åº”è¯¥æŠ›å‡ºValueError"
        except ValueError as e:
            if "è¾“å…¥æ–‡ä»¶ä¸ºç©º" in str(e):
                print("âœ… ç©ºæ–‡ä»¶é”™è¯¯å¤„ç†æ­£ç¡®")
            else:
                raise
        except Exception as e:
            print(f"âŒ ç©ºæ–‡ä»¶é”™è¯¯å¤„ç†å¤±è´¥: {e}")
            raise
    
    def test_complete_workflow(self) -> None:
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
        print("\n=== æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹ ===")
        
        builder = VectorKnowledgeBaseBuilder(
            input_file=self.test_input_file,
            output_dir=self.test_dir
        )
        
        try:
            # æ‰§è¡Œå®Œæ•´æ„å»ºæµç¨‹
            builder.build()
            
            # éªŒè¯è¾“å‡ºæ–‡ä»¶
            assert os.path.exists(builder.index_file), "ç´¢å¼•æ–‡ä»¶æœªåˆ›å»º"
            assert os.path.exists(builder.metadata_file), "å…ƒæ•°æ®æ–‡ä»¶æœªåˆ›å»º"
            
            # éªŒè¯å¯ä»¥æ­£å¸¸åŠ è½½å’Œä½¿ç”¨
            index = faiss.read_index(builder.index_file)
            with open(builder.metadata_file, 'rb') as f:
                metadata = pickle.load(f)
            
            # æµ‹è¯•æœç´¢åŠŸèƒ½
            query_vector = np.random.rand(1, 384).astype(np.float32)
            distances, indices = index.search(query_vector, k=5)
            
            assert len(distances[0]) == 5, "æœç´¢ç»“æœæ•°é‡é”™è¯¯"
            assert all(0 <= idx < len(metadata) for idx in indices[0]), "ç´¢å¼•è¶…å‡ºèŒƒå›´"
            
            print("âœ… å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            print(f"âŒ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def run_all_tests(self) -> None:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        try:
            print("å¼€å§‹å‘é‡çŸ¥è¯†åº“æ„å»ºåŠŸèƒ½æµ‹è¯•...")
            
            # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
            self.setup_test_environment()
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            self.create_test_data()
            
            # è¿è¡Œå„é¡¹æµ‹è¯•
            vector_array, metadata_list = self.test_data_loading()
            index = self.test_faiss_index_building(vector_array)
            self.test_file_saving_and_loading(index, metadata_list)
            self.test_error_handling()
            self.test_complete_workflow()
            
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å‘é‡çŸ¥è¯†åº“æ„å»ºåŠŸèƒ½æ­£å¸¸")
            
        except Exception as e:
            print(f"\nğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
            raise
        finally:
            # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
            self.cleanup_test_environment()


def main():
    """ä¸»å‡½æ•°"""
    tester = TestVectorKnowledgeBaseBuilder()
    tester.run_all_tests()


if __name__ == "__main__":
    main()